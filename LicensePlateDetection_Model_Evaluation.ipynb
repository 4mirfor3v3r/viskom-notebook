{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License Plate Detector using MobilenetV3Small SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.ones(num_objs, dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor(img_id)\n",
    "        # Size of bbox (Rectangular)\n",
    "        areas = []\n",
    "        for i in range(num_objs):\n",
    "            areas.append(coco_annotation[i]['area'])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        # Iscrowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        if num_objs == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            areas = torch.zeros((0,), dtype=torch.float32)\n",
    "            iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            if not isinstance(img, Image.Image):\n",
    "                img = Image.fromarray(img)\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # if train:\n",
    "    #     transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    # rescale to 0-1\n",
    "    transforms.append(T.ToDtype(torch.float, scale=1/255.0))\n",
    "    # transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(torchvision.transforms.ToTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# path to your own data and coco file\n",
    "import utils\n",
    "train_data_dir = 'data/train'\n",
    "train_coco = 'data/train/_annotations.coco.json'\n",
    "\n",
    "test_data_dir = 'data/test'\n",
    "test_coco = 'data/test/_annotations.coco.json'\n",
    "\n",
    "valid_data_dir = 'data/valid'\n",
    "valid_coco = 'data/valid/_annotations.coco.json'\n",
    "\n",
    "# create own Dataset\n",
    "train_ds = CustomDataset(root=train_data_dir,\n",
    "                          annotation=train_coco,\n",
    "                          transforms=get_transform(train=True)\n",
    "                          )\n",
    "\n",
    "test_ds = CustomDataset(root=test_data_dir,\n",
    "                          annotation=test_coco,\n",
    "                          transforms=get_transform(train=False)\n",
    "                          )\n",
    "\n",
    "valid_ds = CustomDataset(root=valid_data_dir,\n",
    "                          annotation=valid_coco,\n",
    "                          transforms=get_transform(train=False)\n",
    "                          )\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Batch size\n",
    "train_batch_size = 4\n",
    "test_batch_size = 4\n",
    "valid_batch_size = 4\n",
    "\n",
    "# own DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_ds,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=utils.collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_ds,\n",
    "                                            batch_size=test_batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            collate_fn=utils.collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(valid_ds,\n",
    "                                            batch_size=valid_batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=\"DEFAULT\")\n",
    "model_resnet = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "in_features_resnet = model_resnet.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model_resnet.roi_heads.box_predictor = FastRCNNPredictor(in_features_resnet, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone_large = torchvision.models.mobilenet_v3_large(weights=\"DEFAULT\").features\n",
    "backbone_large.out_channels = 960\n",
    "\n",
    "backbone = torchvision.models.mobilenet_v3_small(weights=\"DEFAULT\").features\n",
    "\n",
    "# ``FasterRCNN`` needs to know the number of\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 576 \n",
    "\n",
    "anchor_generator_old = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    featmap_names=['0'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# put the pieces together inside a Faster-RCNN model\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=2,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "def getIoU(bbox, gt):\n",
    "    x1, y1, w1, h1 = bbox\n",
    "    x2, y2, w2, h2 = gt\n",
    "    xA = max(x1, x2)\n",
    "    yA = max(y1, y2)\n",
    "    xB = min(x1+w1, x2+w2)\n",
    "    yB = min(y1+h1, y2+h2)\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = w1 * h1\n",
    "    boxBArea = w2 * h2\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "# mean average precision\n",
    "def get_mAP(pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels):\n",
    "    # get mAP\n",
    "    pred = [{'boxes': pred_boxes, 'labels': pred_labels, 'scores': pred_scores}]\n",
    "    gt = [{'boxes': gt_boxes, 'labels': gt_labels}]\n",
    "    map_metric = MeanAveragePrecision(iou_thresholds=[0.5], class_metrics=True)\n",
    "    map_metric.update(pred, gt)\n",
    "    mAP = map_metric.compute()\n",
    "    return mAP['map']\n",
    "\n",
    "\n",
    "def validate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    average_IoU = 0\n",
    "\n",
    "    mAP = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            pred = model(images)\n",
    "            pred_boxes = pred[0]['boxes'].cpu()\n",
    "            pred_labels = pred[0]['labels'].cpu()\n",
    "            pred_scores = pred[0]['scores'].cpu()\n",
    "            \n",
    "            gt_boxes = targets[0]['boxes'].cpu()\n",
    "            gt_labels = targets[0]['labels'].cpu()\n",
    "            # get mAP\n",
    "            if(len(pred_boxes) == 0):\n",
    "                continue\n",
    "            mAP += get_mAP(pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels)\n",
    "            pred_boxes = pred[0]['boxes'].cpu().numpy()\n",
    "            pred_labels = pred[0]['labels'].cpu().numpy()\n",
    "            pred_scores = pred[0]['scores'].cpu().numpy()\n",
    "\n",
    "            gt_boxes = targets[0]['boxes'].cpu().numpy()\n",
    "            gt_labels = targets[0]['labels'].cpu().numpy()\n",
    "\n",
    "            # get IoU\n",
    "            if(len(pred_boxes) == 0):\n",
    "                continue\n",
    "            iou = getIoU(pred_boxes[0], gt_boxes[0])\n",
    "            average_IoU += iou\n",
    "\n",
    "    average_IoU /= len(data_loader)\n",
    "    mAP /= len(data_loader)\n",
    "\n",
    "    return average_IoU, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Axioo Pongo\\AppData\\Local\\Temp\\ipykernel_416\\1763018824.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  small_resnet_model.load_state_dict(torch.load('FasterRCNN_MobileNetV3_small_0.86.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model small loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Axioo Pongo\\AppData\\Local\\Temp\\ipykernel_416\\1763018824.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  large_resnet_model.load_state_dict(torch.load('FasterRCNN_MobileNetV3_large_0.877.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model large loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Axioo Pongo\\AppData\\Local\\Temp\\ipykernel_416\\1763018824.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  small_fpn_model.load_state_dict(torch.load('FasterRCNN_MobileNetV3_320FPN_2_large_0.902.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fpn loaded\n"
     ]
    }
   ],
   "source": [
    "small_resnet_model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=2,\n",
    "    rpn_anchor_generator=anchor_generator_old,\n",
    "    box_roi_pool=roi_pooler\n",
    ").to(device)\n",
    "small_resnet_model.load_state_dict(torch.load('FasterRCNN_MobileNetV3_small_0.86.pth'))\n",
    "small_resnet_model.eval()\n",
    "print(\"Model small loaded\")\n",
    "\n",
    "large_resnet_model = FasterRCNN(\n",
    "    backbone_large,\n",
    "    num_classes=2,\n",
    "    rpn_anchor_generator=anchor_generator_old,\n",
    "    box_roi_pool=roi_pooler\n",
    ").to(device)\n",
    "large_resnet_model.load_state_dict(torch.load('FasterRCNN_MobileNetV3_large_0.877.pth'))\n",
    "large_resnet_model.eval()\n",
    "print(\"Model large loaded\")\n",
    "\n",
    "small_fpn_model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=2,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ").to(device)\n",
    "small_fpn_model.load_state_dict(torch.load('FasterRCNN_MobileNetV3_320FPN_2_large_0.902.pth'))\n",
    "small_fpn_model.eval()\n",
    "print(\"Model fpn loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate small\n",
      "Evaluate large\n",
      "Evaluate fpn\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Evaluate small\")\n",
    "ds_length = len(train_loader.dataset)\n",
    "vit_tic = time.time()\n",
    "for i, (images, targets) in enumerate(train_loader):\n",
    "    with torch.no_grad():\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        small_resnet_model(images)\n",
    "vit_toc = time.time()\n",
    "\n",
    "print(\"Evaluate large\")\n",
    "evit_tic = time.time()\n",
    "for i, (images, targets) in enumerate(train_loader):\n",
    "    with torch.no_grad():\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        large_resnet_model(images)\n",
    "evit_toc = time.time()\n",
    "\n",
    "print(\"Evaluate fpn\")\n",
    "fpn_tic = time.time()\n",
    "for i, (images, targets) in enumerate(train_loader):\n",
    "    with torch.no_grad():\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        small_fpn_model(images)\n",
    "fpn_toc = time.time()\n",
    "\n",
    "print(\"Evaluate large\")\n",
    "fpn_large_tic = time.time()\n",
    "for i, (images, targets) in enumerate(train_loader):\n",
    "    with torch.no_grad():\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        small_fpn_model(images)\n",
    "fpn_large_toc = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:  1203\n",
      "ViT Inference Time:  28 Seconds\n",
      "EfficientViT Inference Time:  55 Seconds\n",
      "FPN Inference Time:  27 Seconds\n",
      "Resnet50 Mobilenet Small Inference Speed: 42.1 images/second\n",
      "Resnet50 Mobilenet Large Inference Speed: 21.6 images/second\n",
      "FPN Mobilenet Small Inference Speed: 44.0 images/second\n",
      "-----------------------------------------------------------\n",
      "FPN is the fastest model\n"
     ]
    }
   ],
   "source": [
    "vit_seconds = int(vit_toc - vit_tic)\n",
    "evit_seconds = int(evit_toc - evit_tic)\n",
    "fpn_seconds = int(fpn_toc - fpn_tic)\n",
    "\n",
    "vit_img_per_second = ds_length / float(vit_seconds)\n",
    "evit_img_per_second = ds_length / float(evit_seconds)\n",
    "fpn_img_per_second = ds_length / float(fpn_seconds)\n",
    "\n",
    "print(\"Total images: \", ds_length)\n",
    "print(\"ViT Inference Time: \", int(vit_toc - vit_tic), \"Seconds\")\n",
    "print(\"EfficientViT Inference Time: \", int(evit_toc - evit_tic), \"Seconds\")\n",
    "print(\"FPN Inference Time: \", int(fpn_toc - fpn_tic), \"Seconds\")\n",
    "\n",
    "# img/seconds\n",
    "print(\"Resnet50 Mobilenet Small Inference Speed: {0:.1f} images/second\".format(ds_length / float(vit_toc - vit_tic)))\n",
    "print(\"Resnet50 Mobilenet Large Inference Speed: {0:.1f} images/second\".format(ds_length / float(evit_toc - evit_tic)))\n",
    "print(\"FPN Mobilenet Small Inference Speed: {0:.1f} images/second\".format(ds_length / float(fpn_toc - fpn_tic)))\n",
    "print(\"-----------------------------------------------------------\")\n",
    "if vit_seconds < evit_seconds and vit_seconds < fpn_seconds:\n",
    "    print(\"ViT is the fastest model\")\n",
    "elif evit_seconds < vit_seconds and evit_seconds < fpn_seconds:\n",
    "    print(\"EfficientViT is the fastest model\")\n",
    "else:\n",
    "    print(\"FPN is the fastest model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "lua"
    }
   },
   "outputs": [],
   "source": [
    "vit_seconds = int(vit_toc - vit_tic)\n",
    "evit_seconds = int(evit_toc - evit_tic)\n",
    "fpn_seconds = int(fpn_toc - fpn_tic)\n",
    "\n",
    "vit_img_per_second = ds_length / float(vit_seconds)\n",
    "evit_img_per_second = ds_length / float(evit_seconds)\n",
    "fpn_img_per_second = ds_length / float(fpn_seconds)\n",
    "\n",
    "print(\"Total images: \", ds_length)\n",
    "print(\"ViT Inference Time: \", int(vit_toc - vit_tic), \"Seconds\")\n",
    "print(\"EfficientViT Inference Time: \", int(evit_toc - evit_tic), \"Seconds\")\n",
    "print(\"FPN Inference Time: \", int(fpn_toc - fpn_tic), \"Seconds\")\n",
    "\n",
    "# img/seconds\n",
    "print(\"ViT Inference Speed: {0:.1f} images/second\".format(ds_length / float(vit_toc - vit_tic)))\n",
    "print(\"EfficientViT Inference Speed: {0:.1f} images/second\".format(ds_length / float(evit_toc - evit_tic)))\n",
    "print(\"FPN Inference Speed: {0:.1f} images/second\".format(ds_length / float(fpn_toc - fpn_tic)))\n",
    "print(\"-----------------------------------------------------------\")\n",
    "if vit_seconds < evit_seconds and vit_seconds < fpn_seconds:\n",
    "    print(\"ViT is the fastest model\")\n",
    "elif evit_seconds < vit_seconds and evit_seconds < fpn_seconds:\n",
    "    print(\"EfficientViT is the fastest model\")\n",
    "else:\n",
    "    print(\"FPN is the fastest model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Resnet50 Mobilenet Small: 3.977G, 33.919M\n",
      "Resnet50 Mobilenet Large: 10.635G, 60.569M\n",
      "FPN Mobilenet Small: 3.772G, 33.910M\n"
     ]
    }
   ],
   "source": [
    "from thop import profile, clever_format\n",
    "\n",
    "input = torch.randn(1, 3, 224, 224).to(device)\n",
    "macs_small_resnet, params_small_resnet = profile(small_resnet_model, inputs=(input,))\n",
    "macs_large_resnet, params_large_resnet = profile(large_resnet_model, inputs=(input,))\n",
    "macs_small_fpn, params_small_fpn = profile(small_fpn_model, inputs=(input,))\n",
    "\n",
    "macs_small_resnet, params_small_resnet = clever_format([macs_small_resnet, params_small_resnet], \"%.3f\")\n",
    "macs_large_resnet, params_large_resnet = clever_format([macs_large_resnet, params_large_resnet], \"%.3f\")\n",
    "macs_small_fpn, params_small_fpn = clever_format([macs_small_fpn, params_small_fpn], \"%.3f\")\n",
    "\n",
    "print(f\"Resnet50 Mobilenet Small: {macs_small_fpn}, {params_small_resnet}\")\n",
    "print(f\"Resnet50 Mobilenet Large: {macs_large_resnet}, {params_large_resnet}\")\n",
    "print(f\"FPN Mobilenet Small: {macs_small_resnet}, {params_small_fpn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
